# MiniMind 代码详解（四）：训练流程详解

> 本篇讲解预训练、SFT、LoRA 等训练过程，对应 `trainer/` 目录下的代码。

## 1. 训练的整体流程

无论是预训练还是微调，PyTorch 训练的基本流程都是：

```
初始化模型和优化器
    ↓
for epoch in epochs:
    for batch in dataloader:
        ↓
    1. 前向传播：output = model(input)
        ↓
    2. 计算损失：loss = criterion(output, target)
        ↓
    3. 反向传播：loss.backward()
        ↓
    4. 更新参数：optimizer.step()
        ↓
    5. 清零梯度：optimizer.zero_grad()
```

## 2. 预训练（train_pretrain.py）

### 2.1 完整代码结构

```python
# trainer/train_pretrain.py 主流程

# ========== 1. 初始化环境 ==========
local_rank = init_distributed_mode()  # 分布式训练初始化
setup_seed(42)  # 设置随机种子，保证可复现

# ========== 2. 模型配置 ==========
lm_config = MiniMindConfig(
    hidden_size=512,       # 隐藏层维度
    num_hidden_layers=8,   # 层数
    use_moe=False          # 是否使用 MoE
)

# ========== 3. 混合精度设置 ==========
dtype = torch.bfloat16  # 使用 bf16 节省显存
autocast_ctx = torch.cuda.amp.autocast(dtype=dtype)

# ========== 4. 模型和数据 ==========
model, tokenizer = init_model(lm_config, from_weight='none')
train_ds = PretrainDataset(data_path, tokenizer, max_length=340)
optimizer = optim.AdamW(model.parameters(), lr=5e-4)

# ========== 5. 训练循环 ==========
for epoch in range(epochs):
    for step, (input_ids, labels) in enumerate(loader):
        train_step(...)
```

### 2.2 训练步骤详解

```python
# trainer/train_pretrain.py 第23-71行
def train_epoch(epoch, loader, iters, start_step=0, wandb=None):
    for step, (input_ids, labels) in enumerate(loader):
        input_ids = input_ids.to(device)  # 数据移到GPU
        labels = labels.to(device)

        # 动态学习率（余弦退火）
        lr = get_lr(epoch * iters + step, epochs * iters, learning_rate)
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        # ===== 核心训练步骤 =====
        with autocast_ctx:  # 混合精度
            res = model(input_ids, labels=labels)  # 前向传播
            loss = res.loss + res.aux_loss         # 损失 = 主损失 + MoE辅助损失
            loss = loss / accumulation_steps       # 梯度累积

        scaler.scale(loss).backward()  # 反向传播（带梯度缩放）

        if (step + 1) % accumulation_steps == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  # 梯度裁剪
            scaler.step(optimizer)     # 更新参数
            scaler.update()
            optimizer.zero_grad()      # 清零梯度
```

### 2.3 关键技术解释

#### 混合精度训练（AMP）

```python
# 使用 bfloat16 减少显存占用，加速训练
autocast_ctx = torch.cuda.amp.autocast(dtype=torch.bfloat16)
scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))

with autocast_ctx:
    output = model(input)  # 前向传播用低精度
    loss = ...

scaler.scale(loss).backward()  # 梯度缩放，防止下溢
scaler.step(optimizer)
scaler.update()
```

**为什么用混合精度？**
- fp32（32位）：精度高但占显存
- bf16（16位）：显存减半，速度翻倍
- 关键计算用 fp32，其他用 bf16

#### 梯度累积

```python
loss = loss / accumulation_steps  # 累积8步的梯度

if (step + 1) % accumulation_steps == 0:
    optimizer.step()  # 每8步才更新一次参数
```

**为什么需要梯度累积？**
- 显存不够用大 batch_size
- 累积8步相当于 batch_size × 8
- 大 batch 训练更稳定

#### 学习率调度（余弦退火）

```python
# trainer/trainer_utils.py 第40-41行
def get_lr(current_step, total_steps, lr):
    return lr * (0.1 + 0.45 * (1 + math.cos(math.pi * current_step / total_steps)))
```

```
学习率变化曲线：

    lr │    ╭──╮
       │   ╱    ╲
       │  ╱      ╲
       │ ╱        ╲
0.1*lr │╱          ╲
       └────────────→ steps
       0           total
```

**为什么用余弦退火？**
- 开始时学习率高，快速学习
- 结束时学习率低，精细调整
- 平滑过渡，训练更稳定

#### 梯度裁剪

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**为什么需要梯度裁剪？**
- 防止梯度爆炸（梯度值过大）
- 限制梯度的最大范数为 1.0
- 保证训练稳定性

## 3. 监督微调（train_full_sft.py）

### 3.1 与预训练的区别

| 方面 | 预训练 | SFT |
|------|--------|-----|
| 数据 | 纯文本 | 对话数据 |
| 损失计算 | 全部token | 只有assistant |
| 学习率 | 5e-4 | 1e-6（更小） |
| 初始权重 | 随机 | 预训练权重 |

### 3.2 关键代码

```python
# trainer/train_full_sft.py

# 从预训练权重开始
model, tokenizer = init_model(lm_config, from_weight='pretrain')

# 使用 SFT 数据集（只计算 assistant 部分的损失）
train_ds = SFTDataset(data_path, tokenizer, max_length=340)

# 使用更小的学习率
optimizer = optim.AdamW(model.parameters(), lr=1e-6)
```

### 3.3 为什么 SFT 学习率要小？

预训练已经学到了语言知识，SFT 只是"微调"：
- 学习率太大会"忘记"预训练知识（灾难性遗忘）
- 学习率太小学不会对话格式
- 1e-6 是一个平衡点

## 4. LoRA 微调（train_lora.py）

### 4.1 LoRA 原理

LoRA（Low-Rank Adaptation）是一种高效微调技术：

```
原始权重 W (512 × 512) 冻结不动
        ↓
添加低秩矩阵 A (512 × 8) 和 B (8 × 512)
        ↓
新输出 = W × x + A × B × x
              ↑
         只训练这部分
```

**为什么 LoRA 高效？**
- 原始参数：512 × 512 = 262,144
- LoRA 参数：512 × 8 + 8 × 512 = 8,192
- 只训练 3% 的参数！

### 4.2 LoRA 代码实现

```python
# model/model_lora.py 第6-18行
class LoRA(nn.Module):
    def __init__(self, in_features, out_features, rank):
        super().__init__()
        self.rank = rank
        self.A = nn.Linear(in_features, rank, bias=False)   # 降维
        self.B = nn.Linear(rank, out_features, bias=False)  # 升维

        # 初始化：A 高斯，B 全零
        self.A.weight.data.normal_(mean=0.0, std=0.02)
        self.B.weight.data.zero_()  # 初始时 LoRA 不改变原始输出

    def forward(self, x):
        return self.B(self.A(x))  # 低秩变换
```

### 4.3 应用 LoRA

```python
# model/model_lora.py 第21-32行
def apply_lora(model, rank=8):
    for name, module in model.named_modules():
        # 只给方阵 Linear 层加 LoRA（如 QKV 投影）
        if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]:
            lora = LoRA(module.weight.shape[0], module.weight.shape[1], rank=rank)
            setattr(module, "lora", lora)

            original_forward = module.forward

            def forward_with_lora(x, layer1=original_forward, layer2=lora):
                return layer1(x) + layer2(x)  # 原始输出 + LoRA 输出

            module.forward = forward_with_lora
```

### 4.4 LoRA 训练流程

```python
# trainer/train_lora.py

# 1. 加载基础模型
model, tokenizer = init_model(lm_config, from_weight='full_sft')

# 2. 应用 LoRA
apply_lora(model, rank=8)

# 3. 冻结原始参数，只训练 LoRA
lora_params = []
for name, param in model.named_parameters():
    if 'lora' in name:
        param.requires_grad = True
        lora_params.append(param)
    else:
        param.requires_grad = False  # 冻结

# 4. 只优化 LoRA 参数
optimizer = optim.AdamW(lora_params, lr=1e-4)

# 5. 保存时只保存 LoRA 权重
save_lora(model, 'lora_weights.pth')  # 很小，只有几MB
```

### 4.5 LoRA 的优势

| 方面 | 全参数微调 | LoRA |
|------|-----------|------|
| 训练参数 | 25M | 0.5M |
| 显存占用 | 高 | 低 |
| 训练速度 | 慢 | 快 |
| 保存大小 | 100MB | 2MB |
| 可切换 | 难 | 易（加载不同LoRA） |

## 5. 分布式训练（DDP）

### 5.1 什么是 DDP？

DDP（Distributed Data Parallel）让多张 GPU 同时训练：

```
GPU 0: 处理 batch 0, 1, 2, 3
GPU 1: 处理 batch 4, 5, 6, 7
GPU 2: 处理 batch 8, 9, 10, 11
GPU 3: 处理 batch 12, 13, 14, 15
        ↓
    梯度同步（AllReduce）
        ↓
    所有 GPU 更新相同的参数
```

### 5.2 DDP 代码

```python
# trainer/trainer_utils.py 第44-51行
def init_distributed_mode():
    if int(os.environ.get("RANK", -1)) == -1:
        return 0  # 单卡模式

    dist.init_process_group(backend="nccl")  # 初始化通信
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    return local_rank
```

```python
# 包装模型
if dist.is_initialized():
    model = DistributedDataParallel(model, device_ids=[local_rank])

# 使用分布式采样器
train_sampler = DistributedSampler(train_ds)
```

### 5.3 启动 DDP 训练

```bash
# 单机4卡
torchrun --nproc_per_node 4 train_pretrain.py

# 多机多卡
torchrun --nnodes 2 --nproc_per_node 4 --node_rank 0 train_pretrain.py
```

## 6. 检查点保存与恢复

### 6.1 保存检查点

```python
# trainer/trainer_utils.py 第63-116行
def lm_checkpoint(lm_config, weight, model, optimizer, epoch, step, ...):
    # 保存模型权重
    state_dict = model.state_dict()
    torch.save({k: v.half().cpu() for k, v in state_dict.items()}, ckp_path)

    # 保存完整恢复信息
    resume_data = {
        'model': state_dict,
        'optimizer': optimizer.state_dict(),
        'epoch': epoch,
        'step': step,
        'world_size': dist.get_world_size()  # GPU数量
    }
    torch.save(resume_data, resume_path)
```

### 6.2 恢复训练

```python
# 检测并加载检查点
ckp_data = lm_checkpoint(lm_config, weight=save_weight) if from_resume else None

if ckp_data:
    model.load_state_dict(ckp_data['model'])
    optimizer.load_state_dict(ckp_data['optimizer'])
    start_epoch = ckp_data['epoch']
    start_step = ckp_data.get('step', 0)
```

### 6.3 GPU 数量变化的处理

```python
# trainer/trainer_utils.py 第110-114行
saved_ws = ckp_data.get('world_size', 1)
current_ws = dist.get_world_size()
if saved_ws != current_ws:
    # 自动调整 step（因为每个GPU处理的数据量变了）
    ckp_data['step'] = ckp_data['step'] * saved_ws // current_ws
```

## 7. 训练监控（WandB/SwanLab）

### 7.1 配置监控

```python
# trainer/train_pretrain.py
if use_wandb:
    import swanlab as wandb
    wandb.init(project="MiniMind-Pretrain", name=run_name)
```

### 7.2 记录指标

```python
if wandb:
    wandb.log({
        "loss": current_loss,
        "logits_loss": current_logits_loss,
        "aux_loss": current_aux_loss,
        "learning_rate": current_lr,
        "epoch_time": eta_min
    })
```

## 8. 完整训练命令

### 8.1 预训练

```bash
cd trainer

# 单卡训练
python train_pretrain.py \
    --epochs 1 \
    --batch_size 32 \
    --learning_rate 5e-4 \
    --hidden_size 512 \
    --num_hidden_layers 8

# 多卡训练
torchrun --nproc_per_node 4 train_pretrain.py
```

### 8.2 SFT

```bash
python train_full_sft.py \
    --epochs 2 \
    --batch_size 16 \
    --learning_rate 1e-6 \
    --from_weight pretrain
```

### 8.3 LoRA

```bash
python train_lora.py \
    --epochs 50 \
    --batch_size 32 \
    --learning_rate 1e-4 \
    --from_weight full_sft \
    --lora_name lora_custom
```

## 9. 总结

| 训练阶段 | 脚本 | 学习率 | 数据 | 参数更新 |
|---------|------|--------|------|---------|
| 预训练 | train_pretrain.py | 5e-4 | 纯文本 | 全部 |
| SFT | train_full_sft.py | 1e-6 | 对话 | 全部 |
| LoRA | train_lora.py | 1e-4 | 对话 | 仅LoRA |

**训练技巧**：
1. 混合精度（bf16）节省显存
2. 梯度累积模拟大 batch
3. 余弦退火学习率调度
4. 梯度裁剪防止爆炸
5. 检查点断点续训

---

[← 上一篇：数据处理详解](./03_数据处理详解.md) | [下一篇：强化学习详解 →](./05_强化学习详解.md)
