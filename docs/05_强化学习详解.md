# MiniMind ä»£ç è¯¦è§£ï¼ˆäº”ï¼‰ï¼šå¼ºåŒ–å­¦ä¹ è¯¦è§£

> æœ¬ç¯‡è®²è§£ DPOã€GRPO ç­‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¯¹åº” `trainer/train_dpo.py` å’Œ `trainer/train_grpo.py`ã€‚

## 1. ä¸ºä»€ä¹ˆéœ€è¦å¼ºåŒ–å­¦ä¹ ï¼Ÿ

ç»è¿‡é¢„è®­ç»ƒå’Œ SFT åï¼Œæ¨¡å‹å·²ç»èƒ½å›ç­”é—®é¢˜äº†ã€‚ä½†ï¼š
- å›ç­”å¯èƒ½ä¸å¤Ÿå‡†ç¡®
- å›ç­”é£æ ¼å¯èƒ½ä¸ç¬¦åˆäººç±»åå¥½
- å¯èƒ½ç”Ÿæˆæœ‰å®³å†…å®¹

**å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡**ï¼šè®©æ¨¡å‹è¾“å‡ºæ›´ç¬¦åˆäººç±»åå¥½çš„å†…å®¹ã€‚

## 2. RLHF çš„ä¸‰ä¸ªé˜¶æ®µ

```
é˜¶æ®µ1: é¢„è®­ç»ƒ + SFT
      â†“
é˜¶æ®µ2: è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰
   - äººç±»æ ‡æ³¨ï¼šå“ªä¸ªå›ç­”æ›´å¥½
   - è®­ç»ƒæ¨¡å‹å­¦ä¹ æ‰“åˆ†
      â†“
é˜¶æ®µ3: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­–ç•¥
   - PPO / DPO / GRPO
   - è®©æ¨¡å‹åå‘é«˜åˆ†å›ç­”
```

## 3. DPOï¼ˆDirect Preference Optimizationï¼‰

### 3.1 DPO çš„æ ¸å¿ƒæ€æƒ³

DPO è·³è¿‡äº†è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥ä»åå¥½æ•°æ®å­¦ä¹ ï¼š

```
ä¼ ç»Ÿ RLHF:  åå¥½æ•°æ® â†’ å¥–åŠ±æ¨¡å‹ â†’ PPOä¼˜åŒ–
DPO:       åå¥½æ•°æ® â†’ ç›´æ¥ä¼˜åŒ–
                      â†‘
                   æ›´ç®€å•ï¼
```

### 3.2 DPO æŸå¤±å‡½æ•°

```python
# trainer/train_dpo.py ç¬¬33-51è¡Œ
def dpo_loss(ref_log_probs, policy_log_probs, mask, beta):
    # 1. è®¡ç®—æ¯ä¸ªåºåˆ—çš„å¹³å‡ log æ¦‚ç‡
    seq_lengths = mask.sum(dim=1, keepdim=True).clamp_min(1e-8)
    ref_log_probs = (ref_log_probs * mask).sum(dim=1) / seq_lengths.squeeze()
    policy_log_probs = (policy_log_probs * mask).sum(dim=1) / seq_lengths.squeeze()

    # 2. åˆ†ç¦» chosen å’Œ rejected
    batch_size = ref_log_probs.shape[0]
    chosen_ref = ref_log_probs[:batch_size // 2]
    reject_ref = ref_log_probs[batch_size // 2:]
    chosen_policy = policy_log_probs[:batch_size // 2]
    reject_policy = policy_log_probs[batch_size // 2:]

    # 3. è®¡ç®— DPO æŸå¤±
    pi_logratios = chosen_policy - reject_policy    # ç­–ç•¥æ¨¡å‹çš„åå¥½
    ref_logratios = chosen_ref - reject_ref         # å‚è€ƒæ¨¡å‹çš„åå¥½
    logits = pi_logratios - ref_logratios           # ç›¸å¯¹å˜åŒ–

    loss = -F.logsigmoid(beta * logits)  # sigmoid æŸå¤±
    return loss.mean()
```

### 3.3 DPO å…¬å¼è§£è¯»

```
DPO Loss = -log(Ïƒ(Î² * (log Ï€(y_w|x)/Ï€(y_l|x) - log Ï€_ref(y_w|x)/Ï€_ref(y_l|x))))

å…¶ä¸­ï¼š
- Ï€: æ­£åœ¨è®­ç»ƒçš„ç­–ç•¥æ¨¡å‹
- Ï€_ref: å‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“çš„SFTæ¨¡å‹ï¼‰
- y_w: å¥½çš„å›ç­”ï¼ˆchosenï¼‰
- y_l: å·®çš„å›ç­”ï¼ˆrejectedï¼‰
- Î²: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åç¦»ç¨‹åº¦
```

**ç›´è§‰ç†è§£**ï¼š
- è®©æ¨¡å‹æ›´åå‘ chosenï¼ˆå¢åŠ å…¶æ¦‚ç‡ï¼‰
- è®©æ¨¡å‹è¿œç¦» rejectedï¼ˆé™ä½å…¶æ¦‚ç‡ï¼‰
- ä½†ä¸èƒ½åç¦» ref_model å¤ªå¤šï¼ˆé˜²æ­¢é—å¿˜ï¼‰

### 3.4 DPO è®­ç»ƒæµç¨‹

```python
# trainer/train_dpo.py ç¬¬54-121è¡Œ
def train_epoch(epoch, loader, ref_model, beta=0.1):
    for step, batch in enumerate(loader):
        # 1. å‡†å¤‡æ•°æ®
        x_chosen = batch['x_chosen'].to(device)
        x_rejected = batch['x_rejected'].to(device)
        x = torch.cat([x_chosen, x_rejected], dim=0)
        y = torch.cat([y_chosen, y_rejected], dim=0)
        mask = torch.cat([mask_chosen, mask_rejected], dim=0)

        # 2. è®¡ç®—å‚è€ƒæ¨¡å‹çš„ log æ¦‚ç‡ï¼ˆä¸æ›´æ–°æ¢¯åº¦ï¼‰
        with torch.no_grad():
            ref_logits = ref_model(x).logits
            ref_log_probs = logits_to_log_probs(ref_logits, y)

        # 3. è®¡ç®—ç­–ç•¥æ¨¡å‹çš„ log æ¦‚ç‡
        logits = model(x).logits
        policy_log_probs = logits_to_log_probs(logits, y)

        # 4. è®¡ç®— DPO æŸå¤±å¹¶æ›´æ–°
        loss = dpo_loss(ref_log_probs, policy_log_probs, mask, beta)
        loss.backward()
        optimizer.step()
```

### 3.5 log_probs è®¡ç®—

```python
# trainer/train_dpo.py ç¬¬24-30è¡Œ
def logits_to_log_probs(logits, labels):
    # logits: [batch, seq_len, vocab_size]
    # labels: [batch, seq_len]

    log_probs = F.log_softmax(logits, dim=2)  # è½¬æˆ log æ¦‚ç‡
    # å–å‡ºæ¯ä¸ªä½ç½®çœŸå®æ ‡ç­¾å¯¹åº”çš„ log æ¦‚ç‡
    log_probs_per_token = torch.gather(log_probs, dim=2, index=labels.unsqueeze(2)).squeeze(-1)
    return log_probs_per_token  # [batch, seq_len]
```

**ç›´è§‰**ï¼š
- æ¨¡å‹å¯¹ä¸€ä¸ªåºåˆ—çš„"æ¦‚ç‡"= æ¯ä¸ª token æ¦‚ç‡çš„ä¹˜ç§¯
- log æ¦‚ç‡çš„å¥½å¤„ï¼šä¹˜æ³•å˜åŠ æ³•ï¼Œæ•°å€¼æ›´ç¨³å®š

## 4. GRPOï¼ˆGroup Relative Policy Optimizationï¼‰

### 4.1 GRPO çš„æ ¸å¿ƒæ€æƒ³

GRPO æ˜¯ DeepSeek-R1 ä½¿ç”¨çš„ç®—æ³•ï¼Œä¸éœ€è¦å•ç‹¬çš„å¥–åŠ±æ¨¡å‹ï¼š

```
ä¼ ç»Ÿ PPO:  ç­–ç•¥æ¨¡å‹ + ä»·å€¼æ¨¡å‹ + å¥–åŠ±æ¨¡å‹ï¼ˆ3ä¸ªæ¨¡å‹ï¼‰
GRPO:     ç­–ç•¥æ¨¡å‹ + å‚è€ƒæ¨¡å‹ + ç»„å†…ç›¸å¯¹å¥–åŠ±ï¼ˆ2ä¸ªæ¨¡å‹ï¼‰
                               â†‘
                         å…³é”®åˆ›æ–°ï¼
```

### 4.2 GRPO çš„å·¥ä½œæµç¨‹

```
1. ç»™åŒä¸€ä¸ª prompt ç”Ÿæˆ N ä¸ªå›ç­”ï¼ˆnum_generations=8ï¼‰
           â†“
2. ç”¨å¥–åŠ±æ¨¡å‹ç»™æ¯ä¸ªå›ç­”æ‰“åˆ†
           â†“
3. è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼ˆAdvantageï¼‰
   - åˆ†æ•°é«˜äºå¹³å‡ â†’ æ­£ä¼˜åŠ¿ â†’ å¢åŠ æ¦‚ç‡
   - åˆ†æ•°ä½äºå¹³å‡ â†’ è´Ÿä¼˜åŠ¿ â†’ é™ä½æ¦‚ç‡
           â†“
4. ç”¨ç­–ç•¥æ¢¯åº¦æ›´æ–°æ¨¡å‹
```

### 4.3 å¥–åŠ±è®¡ç®—

```python
# trainer/train_grpo.py ç¬¬27-92è¡Œ
def calculate_rewards(prompts, responses, reward_model, reward_tokenizer):
    rewards = torch.zeros(len(responses), device=device)

    # 1. æ ¼å¼å¥–åŠ±ï¼ˆæ¨ç†æ¨¡å‹ä¸“ç”¨ï¼‰
    if reasoning:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ­£ç¡®çš„ <think></think><answer></answer> æ ¼å¼
        pattern = r"^<think>\n.*?\n</think>\n<answer>\n.*?\n</answer>$"
        for i, response in enumerate(responses):
            if re.match(pattern, response, re.S):
                rewards[i] += 0.5  # æ ¼å¼æ­£ç¡®åŠ åˆ†

            # æ ‡ç­¾æ•°é‡æ£€æŸ¥
            if response.count("<think>") == 1: rewards[i] += 0.25
            if response.count("</think>") == 1: rewards[i] += 0.25
            if response.count("<answer>") == 1: rewards[i] += 0.25
            if response.count("</answer>") == 1: rewards[i] += 0.25

    # 2. å¥–åŠ±æ¨¡å‹æ‰“åˆ†
    for i, (prompt, response) in enumerate(zip(prompts, responses)):
        score = reward_model.get_score(reward_tokenizer, messages + [{"role": "assistant", "content": response}])
        score = max(min(score, 3.0), -3.0)  # é™åˆ¶èŒƒå›´
        rewards[i] += score

    return rewards
```

### 4.4 ä¼˜åŠ¿å‡½æ•°è®¡ç®—

```python
# trainer/train_grpo.py ç¬¬133-137è¡Œ
# è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿
grouped_rewards = rewards.view(-1, num_generations)  # [B, 8]

mean_r = grouped_rewards.mean(dim=1)  # ç»„å†…å¹³å‡
std_r = grouped_rewards.std(dim=1)    # ç»„å†…æ ‡å‡†å·®

# æ ‡å‡†åŒ–ä¼˜åŠ¿
advantages = (rewards - mean_r) / (std_r + 1e-4)
advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
```

**ç›´è§‰**ï¼š
```
å‡è®¾ 8 ä¸ªå›ç­”çš„åˆ†æ•°æ˜¯ï¼š[3, 5, 2, 8, 4, 6, 1, 7]
å¹³å‡åˆ†ï¼š4.5
æ ‡å‡†å·®ï¼š2.29

ä¼˜åŠ¿å€¼ï¼š
å›ç­”1: (3-4.5)/2.29 = -0.65 â†’ é™ä½æ¦‚ç‡
å›ç­”2: (5-4.5)/2.29 = +0.22 â†’ ç•¥å¾®æå‡
å›ç­”4: (8-4.5)/2.29 = +1.53 â†’ å¤§å¹…æå‡
å›ç­”7: (1-4.5)/2.29 = -1.53 â†’ å¤§å¹…é™ä½
```

### 4.5 ç­–ç•¥æŸå¤±è®¡ç®—

```python
# trainer/train_grpo.py ç¬¬139-148è¡Œ
# 1. è®¡ç®— KL æ•£åº¦æƒ©ç½š
kl_div = ref_per_token_logps - per_token_logps
per_token_kl = torch.exp(kl_div) - kl_div - 1  # è¿‘ä¼¼ KL

# 2. è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
per_token_loss = -(
    torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)
    - beta * per_token_kl  # KL æƒ©ç½š
)

# 3. åªåœ¨æœ‰æ•ˆ token ä¸Šè®¡ç®—
policy_loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
```

### 4.6 GRPO å…¬å¼

```
L_GRPO = -E[ Î£_t ( exp(log Ï€(a_t|s_t) - log Ï€_old(a_t|s_t)) * A_t ) - Î² * KL(Ï€ || Ï€_ref) ]

å…¶ä¸­ï¼š
- Ï€: ç­–ç•¥æ¨¡å‹ï¼ˆæ­£åœ¨æ›´æ–°ï¼‰
- Ï€_old: ç­–ç•¥æ¨¡å‹ï¼ˆæ›´æ–°å‰ï¼Œdetachï¼‰
- Ï€_ref: å‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“ï¼‰
- A_t: ä¼˜åŠ¿å€¼
- Î²: KL æƒ©ç½šç³»æ•°
```

## 5. PPO ç®€ä»‹

MiniMind ä¹Ÿå®ç°äº† PPOï¼ˆ`train_ppo.py`ï¼‰ï¼Œè¿™æ˜¯æ›´ä¼ ç»Ÿçš„ RLHF æ–¹æ³•ï¼š

```
PPO çš„ç»„ä»¶ï¼š
1. Actorï¼ˆç­–ç•¥æ¨¡å‹ï¼‰ï¼šç”Ÿæˆå›ç­”
2. Criticï¼ˆä»·å€¼æ¨¡å‹ï¼‰ï¼šä¼°è®¡çŠ¶æ€ä»·å€¼
3. Reward Modelï¼šæ‰“åˆ†

PPO çš„ä¼˜åŒ–ç›®æ ‡ï¼š
- æœ€å¤§åŒ–å¥–åŠ±
- é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼ˆclipï¼‰
- ä»·å€¼å‡½æ•°é€¼è¿‘
```

**PPO vs GRPO**ï¼š
| æ–¹é¢ | PPO | GRPO |
|------|-----|------|
| æ¨¡å‹æ•°é‡ | 3ä¸ª | 2ä¸ª |
| æ˜¾å­˜å ç”¨ | é«˜ | ä½ |
| å®ç°å¤æ‚åº¦ | é«˜ | ä½ |
| æ•ˆæœ | å¥½ | ç›¸å½“ |

## 6. æ¨ç†æ¨¡å‹è®­ç»ƒï¼ˆtrain_reason.pyï¼‰

MiniMind è¿˜æ”¯æŒè®­ç»ƒç±»ä¼¼ DeepSeek-R1 çš„æ¨ç†æ¨¡å‹ï¼š

### 6.1 æ¨ç†æ¨¡å‹çš„ç‰¹ç‚¹

```
æ™®é€šæ¨¡å‹å›ç­”ï¼š
User: 1+1=?
Assistant: 1+1=2

æ¨ç†æ¨¡å‹å›ç­”ï¼š
User: 1+1=?
Assistant: <think>
è®©æˆ‘æ¥è®¡ç®—è¿™ä¸ªåŠ æ³•é—®é¢˜ã€‚
1+1 æ˜¯ä¸€ä¸ªåŸºæœ¬çš„åŠ æ³•è¿ç®—ã€‚
1 åŠ ä¸Š 1 ç­‰äº 2ã€‚
</think>
<answer>
1+1=2
</answer>
```

### 6.2 æ ¼å¼å¥–åŠ±çš„ä½œç”¨

```python
# trainer/train_grpo.py ç¬¬29-52è¡Œ
def reasoning_model_reward(rewards):
    # æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®
    pattern = r"^<think>\n.*?\n</think>\n<answer>\n.*?\n</answer>$"

    for i, response in enumerate(responses):
        if re.match(pattern, response, re.S):
            rewards[i] += 0.5  # æ ¼å¼æ­£ç¡® +0.5

        # æ ‡ç­¾å®Œæ•´æ€§æ£€æŸ¥
        if response.count("<think>") == 1: rewards[i] += 0.25
        if response.count("</think>") == 1: rewards[i] += 0.25
        if response.count("<answer>") == 1: rewards[i] += 0.25
        if response.count("</answer>") == 1: rewards[i] += 0.25

    return rewards  # æœ€é«˜å¯å¾— 1.5 åˆ†æ ¼å¼å¥–åŠ±
```

## 7. å¼ºåŒ–å­¦ä¹ çš„å…³é”®å‚æ•°

### 7.1 DPO å‚æ•°

```python
# trainer/train_dpo.py å‚æ•°
--beta 0.1           # KL æƒ©ç½šç³»æ•°ï¼ˆè¶Šå¤§è¶Šä¿å®ˆï¼‰
--learning_rate 4e-8 # å­¦ä¹ ç‡ï¼ˆéå¸¸å°ï¼ï¼‰
--batch_size 4       # å° batchï¼ˆæ˜¾å­˜é™åˆ¶ï¼‰
```

### 7.2 GRPO å‚æ•°

```python
# trainer/train_grpo.py å‚æ•°
--num_generations 8  # æ¯ä¸ªpromptç”Ÿæˆ8ä¸ªå›ç­”
--beta 0.02          # KL æƒ©ç½šç³»æ•°
--max_gen_len 1536   # æœ€å¤§ç”Ÿæˆé•¿åº¦
--learning_rate 8e-8 # å­¦ä¹ ç‡
```

### 7.3 ä¸ºä»€ä¹ˆå¼ºåŒ–å­¦ä¹ çš„å­¦ä¹ ç‡è¿™ä¹ˆå°ï¼Ÿ

```
é¢„è®­ç»ƒå­¦ä¹ ç‡ï¼š   5e-4
SFT å­¦ä¹ ç‡ï¼š     1e-6
DPO/GRPO å­¦ä¹ ç‡ï¼š4e-8 ~ 8e-8

åŸå› ï¼š
1. æ¨¡å‹å·²ç»å¾ˆå¥½äº†ï¼Œåªéœ€è¦å¾®è°ƒ
2. å¼ºåŒ–å­¦ä¹ ä¿¡å·å™ªå£°å¤§ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
3. é˜²æ­¢ç¾éš¾æ€§é—å¿˜
```

## 8. å®è·µå»ºè®®

### 8.1 é€‰æ‹©å“ªç§æ–¹æ³•ï¼Ÿ

| åœºæ™¯ | æ¨èæ–¹æ³• | åŸå›  |
|------|----------|------|
| æœ‰åå¥½æ•°æ® | DPO | ç®€å•é«˜æ•ˆ |
| è¦è®­ç»ƒæ¨ç†æ¨¡å‹ | GRPO | æ”¯æŒæ ¼å¼å¥–åŠ± |
| æ˜¾å­˜å……è¶³ | PPO | æ•ˆæœæœ€å¥½ |
| å¿«é€Ÿå®éªŒ | DPO | ä¸éœ€è¦å¥–åŠ±æ¨¡å‹ |

### 8.2 æ•°æ®å‡†å¤‡

**DPO æ•°æ®**ï¼š
```json
{
  "chosen": [é—®é¢˜ + å¥½å›ç­”],
  "rejected": [é—®é¢˜ + å·®å›ç­”]
}
```

**GRPO æ•°æ®**ï¼š
```json
{
  "conversations": [é—®é¢˜]  // åªéœ€è¦é—®é¢˜ï¼Œå›ç­”ç”±æ¨¡å‹ç”Ÿæˆ
}
```

### 8.3 è®­ç»ƒæµç¨‹

```bash
# 1. å…ˆå®Œæˆé¢„è®­ç»ƒå’Œ SFT
python train_pretrain.py
python train_full_sft.py

# 2. DPO è®­ç»ƒ
python train_dpo.py --from_weight full_sft

# æˆ– GRPO è®­ç»ƒï¼ˆéœ€è¦å¥–åŠ±æ¨¡å‹ï¼‰
python train_grpo.py --from_weight full_sft --reward_model_path /path/to/reward_model
```

## 9. æ€»ç»“

| æ–¹æ³• | åŸç† | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|------|
| DPO | ç›´æ¥ä»åå¥½æ•°æ®å­¦ä¹  | ç®€å•ã€æ— éœ€å¥–åŠ±æ¨¡å‹ | éœ€è¦æˆå¯¹æ•°æ® |
| GRPO | ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ | æ•ˆç‡é«˜ã€æ”¯æŒæ¨ç† | éœ€è¦å¥–åŠ±æ¨¡å‹ |
| PPO | Actor-Critic | æ•ˆæœå¥½ | å¤æ‚ã€æ˜¾å­˜é«˜ |

**æ ¸å¿ƒå…¬å¼**ï¼š
- DPO: æœ€å¤§åŒ– log(Ïƒ(Î² * Î”))ï¼Œå…¶ä¸­ Î” = ç­–ç•¥åå¥½ - å‚è€ƒåå¥½
- GRPO: æœ€å¤§åŒ– A * log Ï€ - Î² * KLï¼Œå…¶ä¸­ A æ˜¯ç»„å†…ç›¸å¯¹ä¼˜åŠ¿

---

[â† ä¸Šä¸€ç¯‡ï¼šè®­ç»ƒæµç¨‹è¯¦è§£](./04_è®­ç»ƒæµç¨‹è¯¦è§£.md)

---

## é™„å½•ï¼šå­¦ä¹ è·¯çº¿å›¾

```
ç¬¬1å‘¨ï¼šç†è§£åŸºç¡€
â”œâ”€â”€ é˜…è¯» 01_LLMåŸºç¡€æ¦‚å¿µ.md
â”œâ”€â”€ é˜…è¯» 02_æ¨¡å‹æ¶æ„è¯¦è§£.md
â””â”€â”€ è¿è¡Œ eval_llm.py æµ‹è¯•æ¨¡å‹

ç¬¬2å‘¨ï¼šç†è§£æ•°æ®å’Œè®­ç»ƒ
â”œâ”€â”€ é˜…è¯» 03_æ•°æ®å¤„ç†è¯¦è§£.md
â”œâ”€â”€ é˜…è¯» 04_è®­ç»ƒæµç¨‹è¯¦è§£.md
â””â”€â”€ è¿è¡Œ train_pretrain.pyï¼ˆå°æ•°æ®é›†ï¼‰

ç¬¬3å‘¨ï¼šåŠ¨æ‰‹è®­ç»ƒ
â”œâ”€â”€ ä¸‹è½½å®Œæ•´æ•°æ®é›†
â”œâ”€â”€ å®Œæˆé¢„è®­ç»ƒ + SFT
â””â”€â”€ æµ‹è¯•è‡ªå·±è®­ç»ƒçš„æ¨¡å‹

ç¬¬4å‘¨ï¼šè¿›é˜¶
â”œâ”€â”€ é˜…è¯» 05_å¼ºåŒ–å­¦ä¹ è¯¦è§£.md
â”œâ”€â”€ å°è¯• LoRA å¾®è°ƒ
â””â”€â”€ å°è¯• DPO è®­ç»ƒ

æŒç»­å­¦ä¹ ï¼š
â”œâ”€â”€ é˜…è¯» MiniMind æºç 
â”œâ”€â”€ ä¿®æ”¹æ¨¡å‹ç»“æ„åšå®éªŒ
â””â”€â”€ åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè®­ç»ƒ
```

**ç¥ä½ å­¦ä¹ æ„‰å¿«ï¼** ğŸ‰
