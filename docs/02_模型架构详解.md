# MiniMind 代码详解（二）：模型架构详解

> 本篇深入讲解 Transformer 架构的每个组件，对应 `model/model_minimind.py`。

## 1. 整体架构图

```
输入 Token IDs
      ↓
┌─────────────────────────────────────┐
│         Embedding Layer             │  把ID变成向量
└─────────────────────────────────────┘
      ↓
┌─────────────────────────────────────┐
│       Transformer Block × 8         │  核心计算（重复8次）
│  ┌───────────────────────────────┐  │
│  │      RMSNorm                  │  │  归一化
│  │         ↓                     │  │
│  │   Self-Attention (+ RoPE)     │  │  注意力机制
│  │         ↓                     │  │
│  │      残差连接                  │  │
│  │         ↓                     │  │
│  │      RMSNorm                  │  │  归一化
│  │         ↓                     │  │
│  │   Feed Forward (SwiGLU)       │  │  前馈网络
│  │         ↓                     │  │
│  │      残差连接                  │  │
│  └───────────────────────────────┘  │
└─────────────────────────────────────┘
      ↓
┌─────────────────────────────────────┐
│           RMSNorm                   │  最终归一化
└─────────────────────────────────────┘
      ↓
┌─────────────────────────────────────┐
│         LM Head (Linear)            │  输出词表概率
└─────────────────────────────────────┘
      ↓
预测的下一个词
```

## 2. RMSNorm：归一化层

### 2.1 为什么需要归一化？

神经网络训练时，数据分布会不断变化（内部协变量偏移），导致训练不稳定。归一化让数据保持稳定的分布。

### 2.2 RMSNorm vs LayerNorm

传统的 LayerNorm 需要计算均值和方差，RMSNorm 只计算均方根，更简单高效：

```python
# model/model_minimind.py 第96-106行
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))  # 可学习的缩放参数

    def _norm(self, x):
        # 计算均方根的倒数，然后缩放
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        return self.weight * self._norm(x.float()).type_as(x)
```

**公式解释**：
```
RMSNorm(x) = x / sqrt(mean(x²) + eps) * weight

其中：
- x.pow(2).mean(-1) 计算每个位置的平方均值
- torch.rsqrt() 是 1/sqrt() 的高效实现
- eps 防止除以零
- weight 是可学习的缩放参数
```

## 3. 自注意力机制（Self-Attention）

### 3.1 注意力的直觉理解

想象你在读一句话：`"小明喜欢打篮球，他很擅长。"`

当你理解"他"这个词时，大脑会自动关联到"小明"。这就是注意力机制做的事情——让每个词"看到"其他相关的词。

### 3.2 QKV 机制

```python
# model/model_minimind.py 第150-166行
class Attention(nn.Module):
    def __init__(self, args: MiniMindConfig):
        # Q, K, V 三个投影矩阵
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)
```

**QKV 的含义**：
- **Q (Query)**：当前词想要"查询"什么信息
- **K (Key)**：每个词提供的"索引键"
- **V (Value)**：每个词实际包含的"内容"

```
比喻：图书馆查书
- Q = 你想找什么书（查询）
- K = 每本书的标签（索引）
- V = 书的实际内容（值）

Q 和 K 匹配度高 → 这本书（V）对你更重要
```

### 3.3 注意力计算

```python
# model/model_minimind.py 第196-209行
if self.flash:  # Flash Attention（高效实现）
    output = F.scaled_dot_product_attention(xq, xk, xv, ...)
else:  # 标准实现
    scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # QK相似度
    scores = F.softmax(scores.float(), dim=-1)  # 归一化成概率
    output = scores @ xv  # 用概率加权V
```

**计算步骤**：
```
1. 相似度：scores = Q × K^T / sqrt(d)
   - 矩阵乘法计算每对词的相似度
   - 除以 sqrt(d) 防止数值过大

2. 归一化：weights = softmax(scores)
   - 把相似度变成概率分布（和为1）

3. 加权求和：output = weights × V
   - 相似度高的词贡献更多
```

### 3.4 多头注意力（Multi-Head Attention）

```python
# 把 hidden_size=512 分成 num_attention_heads=8 个头
# 每个头的维度 head_dim = 512/8 = 64
self.head_dim = args.hidden_size // args.num_attention_heads
```

**为什么要多头？**
- 每个头可以关注不同类型的关系
- 头1 可能关注语法关系（主谓宾）
- 头2 可能关注语义关系（同义词）
- 头3 可能关注位置关系（前后文）

### 3.5 因果掩码（Causal Mask）

生成模型只能看到当前位置之前的内容，不能"偷看"后面：

```python
# model/model_minimind.py 第200行
scores[:, :, :, -seq_len:] += torch.triu(
    torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
    diagonal=1
)
```

```
掩码矩阵（4个词的例子）：
        词1   词2   词3   词4
词1  [  0   -inf  -inf  -inf ]  ← 词1只能看自己
词2  [  0     0   -inf  -inf ]  ← 词2能看词1和自己
词3  [  0     0     0   -inf ]  ← 词3能看前三个
词4  [  0     0     0     0  ]  ← 词4能看所有

-inf 经过 softmax 后变成 0，相当于"看不见"
```

### 3.6 GQA（Grouped Query Attention）

MiniMind 使用 GQA 技术减少计算量：

```python
num_attention_heads: int = 8     # Q 有 8 个头
num_key_value_heads: int = 2     # K, V 只有 2 个头
```

**原理**：多个 Q 头共享同一组 K, V，减少内存和计算：

```
标准 MHA：  Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8
           K1 K2 K3 K4 K5 K6 K7 K8
           V1 V2 V3 V4 V5 V6 V7 V8

GQA：      Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8
           K1 K1 K1 K1 K2 K2 K2 K2  ← 4个Q共享1个K
           V1 V1 V1 V1 V2 V2 V2 V2  ← 4个Q共享1个V
```

```python
# model/model_minimind.py 第140-147行
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """把 KV 复制 n_rep 次以匹配 Q 的头数"""
    bs, slen, num_key_value_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return x[:, :, :, None, :].expand(...).reshape(...)
```

## 4. 旋转位置编码（RoPE）

### 4.1 为什么需要位置编码？

Transformer 本身不知道词的顺序。`"猫吃鱼"` 和 `"鱼吃猫"` 对它来说是一样的！位置编码告诉模型每个词在哪个位置。

### 4.2 RoPE 的原理

RoPE 通过旋转向量来编码位置信息：

```python
# model/model_minimind.py 第109-128行
def precompute_freqs_cis(dim, end, rope_base):
    # 生成频率
    freqs = 1.0 / (rope_base ** (torch.arange(0, dim, 2) / dim))
    # 位置序列
    t = torch.arange(end)
    # 外积得到角度矩阵
    freqs = torch.outer(t, freqs).float()
    # 转成 cos 和 sin
    freqs_cos = torch.cos(freqs)
    freqs_sin = torch.sin(freqs)
    return freqs_cos, freqs_sin
```

```python
# model/model_minimind.py 第131-137行
def apply_rotary_pos_emb(q, k, cos, sin):
    def rotate_half(x):
        # 把向量分成两半，交换并取反
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)

    q_embed = (q * cos) + (rotate_half(q) * sin)  # 旋转 Q
    k_embed = (k * cos) + (rotate_half(k) * sin)  # 旋转 K
    return q_embed, k_embed
```

**直觉理解**：
- 位置1的词旋转10度
- 位置2的词旋转20度
- 位置3的词旋转30度
- ...

这样，模型可以通过两个向量的夹角来判断它们的相对位置。

## 5. 前馈网络（FFN）与 SwiGLU

### 5.1 FFN 的作用

注意力层让词互相交流，FFN 让每个词独立"思考"，增加模型的表达能力。

### 5.2 SwiGLU 激活函数

MiniMind 使用 SwiGLU 而非传统的 ReLU：

```python
# model/model_minimind.py 第216-229行
class FeedForward(nn.Module):
    def __init__(self, config):
        # 计算中间层维度（约 hidden_size 的 2.67 倍）
        intermediate_size = int(config.hidden_size * 8 / 3)
        self.gate_proj = nn.Linear(hidden_size, intermediate_size)  # 门控
        self.up_proj = nn.Linear(hidden_size, intermediate_size)    # 上投影
        self.down_proj = nn.Linear(intermediate_size, hidden_size)  # 下投影
        self.act_fn = ACT2FN['silu']  # SiLU 激活

    def forward(self, x):
        # SwiGLU: down(silu(gate(x)) * up(x))
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
```

**SwiGLU 结构**：
```
输入 x (512维)
    ├─→ gate_proj ─→ SiLU ─→ (1365维)
    │                           ↓
    │                          乘法 ─→ down_proj ─→ 输出 (512维)
    │                           ↑
    └─→ up_proj ─────────→ (1365维)
```

**为什么用 SwiGLU？**
- 门控机制让网络学会"选择性激活"
- 实验表明比 ReLU、GELU 效果更好
- LLaMA、PaLM 等主流模型都使用

## 6. MoE（混合专家模型）

MiniMind 还支持 MoE 架构，这是一种稀疏计算技术：

```python
# model/model_minimind.py 第288-326行
class MOEFeedForward(nn.Module):
    def __init__(self, config):
        # 创建多个 FFN "专家"
        self.experts = nn.ModuleList([
            FeedForward(config) for _ in range(config.n_routed_experts)  # 4个专家
        ])
        self.gate = MoEGate(config)  # 门控网络，决定用哪个专家

    def forward(self, x):
        # 1. 门控网络选择 top-k 个专家
        topk_idx, topk_weight, aux_loss = self.gate(x)

        # 2. 只让选中的专家处理数据
        for i, expert in enumerate(self.experts):
            expert_out = expert(x[topk_idx == i])  # 只处理分配给它的数据
```

**MoE 的优势**：
- 总参数量大，但每次只激活一部分（稀疏计算）
- 4个专家，每次只用2个 → 计算量减半，但容量翻倍

## 7. 残差连接

每个子层都有残差连接：

```python
# model/model_minimind.py 第365-372行
class MiniMindBlock(nn.Module):
    def forward(self, hidden_states, ...):
        residual = hidden_states

        # 注意力
        hidden_states, _ = self.self_attn(self.input_layernorm(hidden_states), ...)
        hidden_states += residual  # 残差连接

        # FFN
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))
        return hidden_states
```

**为什么需要残差？**
- 让梯度更容易传播，避免深层网络训练困难
- 让模型可以选择"跳过"某些计算
- 有了残差，模型可以堆叠很多层

## 8. KV Cache（推理优化）

生成文本时，每次只预测一个新词，但需要看到之前所有词。KV Cache 避免重复计算：

```python
# model/model_minimind.py 第184-188行
# kv_cache实现
if past_key_value is not None:
    xk = torch.cat([past_key_value[0], xk], dim=1)  # 拼接历史K
    xv = torch.cat([past_key_value[1], xv], dim=1)  # 拼接历史V
past_kv = (xk, xv) if use_cache else None  # 保存供下次使用
```

**工作原理**：
```
生成 "今天天气真好"

第1步：计算 "今" 的 K1, V1，缓存起来
第2步：计算 "天" 的 K2, V2，与缓存的 K1, V1 拼接
第3步：计算 "天" 的 K3, V3，与 K1, K2, V1, V2 拼接
...

这样每步只需计算新词的 K, V，之前的直接从缓存读取
```

## 9. 完整代码结构图

```
MiniMindForCausalLM
├── model (MiniMindModel)
│   ├── embed_tokens (Embedding)        # Token → 向量
│   ├── dropout (Dropout)               # 防止过拟合
│   ├── layers (ModuleList)             # 8个 Transformer Block
│   │   └── MiniMindBlock × 8
│   │       ├── input_layernorm (RMSNorm)
│   │       ├── self_attn (Attention)
│   │       │   ├── q_proj, k_proj, v_proj, o_proj
│   │       │   └── RoPE 位置编码
│   │       ├── post_attention_layernorm (RMSNorm)
│   │       └── mlp (FeedForward 或 MOEFeedForward)
│   ├── norm (RMSNorm)                  # 最终归一化
│   └── freqs_cos, freqs_sin (Buffer)   # 位置编码缓存
└── lm_head (Linear)                    # 输出层，hidden_size → vocab_size
```

## 10. 总结

| 组件 | 作用 | MiniMind 配置 |
|------|------|--------------|
| Embedding | ID → 向量 | 6400 × 512 |
| RMSNorm | 稳定训练 | eps=1e-5 |
| Attention | 词间交互 | 8头，GQA(2 KV头) |
| RoPE | 位置信息 | base=1e6 |
| FFN/SwiGLU | 非线性变换 | 512 → 1365 → 512 |
| 残差连接 | 梯度传播 | 每个子层 |
| KV Cache | 推理加速 | 自动缓存 |

---

[← 上一篇：LLM 基础概念](./01_LLM基础概念.md) | [下一篇：数据处理详解 →](./03_数据处理详解.md)
