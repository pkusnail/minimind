# MiniMind 代码详解（三）：数据处理详解

> 本篇讲解 LLM 训练数据的格式和处理方式，对应 `dataset/lm_dataset.py`。

## 1. 数据格式概览

MiniMind 使用 JSONL（JSON Lines）格式存储数据，每行一个 JSON 对象：

```jsonl
{"text": "今天天气真好，适合出门散步。"}
{"text": "机器学习是人工智能的一个分支。"}
```

不同训练阶段使用不同的数据格式：

| 阶段 | 数据格式 | 用途 |
|------|----------|------|
| 预训练 | `{"text": "..."}` | 学习语言知识 |
| SFT | `{"conversations": [...]}` | 学习对话格式 |
| DPO | `{"chosen": [...], "rejected": [...]}` | 学习偏好 |
| RLAIF | `{"conversations": [...]}` | 强化学习 |

## 2. 预训练数据集（PretrainDataset）

### 2.1 数据格式

```json
{"text": "人工智能（AI）是计算机科学的一个分支..."}
```

### 2.2 代码解析

```python
# dataset/lm_dataset.py 第7-25行
class PretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        # 加载 JSONL 文件
        self.samples = load_dataset('json', data_files=data_path, split='train')

    def __getitem__(self, index):
        sample = self.samples[index]

        # 1. 分词（不添加特殊token）
        tokens = self.tokenizer(
            str(sample['text']),
            add_special_tokens=False,
            max_length=self.max_length - 2,  # 预留BOS和EOS的位置
            truncation=True
        ).input_ids

        # 2. 添加 BOS（开始）和 EOS（结束）标记
        tokens = [self.tokenizer.bos_token_id] + tokens + [self.tokenizer.eos_token_id]

        # 3. 填充到固定长度
        input_ids = tokens + [self.tokenizer.pad_token_id] * (self.max_length - len(tokens))
        input_ids = torch.tensor(input_ids, dtype=torch.long)

        # 4. 生成标签（与输入相同，但填充位置设为-100）
        labels = input_ids.clone()
        labels[input_ids == self.tokenizer.pad_token_id] = -100

        return input_ids, labels
```

### 2.3 数据处理流程图

```
原始文本: "今天天气真好"
           ↓
分词: [234, 567, 89, 12, 345]
           ↓
添加特殊标记: [BOS, 234, 567, 89, 12, 345, EOS]
           ↓
填充: [BOS, 234, 567, 89, 12, 345, EOS, PAD, PAD, PAD]
           ↓
input_ids: [1, 234, 567, 89, 12, 345, 2, 0, 0, 0]
labels:    [1, 234, 567, 89, 12, 345, 2, -100, -100, -100]
                                        ↑
                                    不计算损失
```

### 2.4 为什么 labels = input_ids？

预训练是"自监督"学习：
- 输入：`[今, 天, 天, 气, 真]`
- 标签：`[天, 天, 气, 真, 好]`（错位一个位置）

模型要学会预测下一个词。这个错位在计算损失时处理（见第一篇）。

## 3. SFT 数据集（SFTDataset）

### 3.1 数据格式

```json
{
  "conversations": [
    {"role": "system", "content": "你是一个有帮助的助手。"},
    {"role": "user", "content": "你好，请介绍一下自己。"},
    {"role": "assistant", "content": "你好！我是一个AI助手..."}
  ]
}
```

### 3.2 Chat Template

SFT 需要把对话转换成特定格式，让模型学会区分用户和助手：

```python
# dataset/lm_dataset.py 第40-48行
def create_chat_prompt(self, cs):
    messages = cs.copy()
    return self.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False
    )
```

转换后的格式（MiniMind 使用的格式）：
```
<|im_start|>system
你是一个有帮助的助手。<|im_end|>
<|im_start|>user
你好，请介绍一下自己。<|im_end|>
<|im_start|>assistant
你好！我是一个AI助手...<|im_end|>
```

### 3.3 关键：只计算 Assistant 回复的损失

SFT 的核心是：模型只需要学习**如何回复**，不需要学习如何提问。

```python
# dataset/lm_dataset.py 第50-66行
def generate_labels(self, input_ids):
    labels = [-100] * len(input_ids)  # 默认全部忽略
    i = 0
    while i < len(input_ids):
        # 找到 "<|im_start|>assistant\n" 的位置
        if input_ids[i:i + len(self.bos_id)] == self.bos_id:
            start = i + len(self.bos_id)
            end = start
            # 找到 "<|im_end|>\n" 的位置
            while end < len(input_ids):
                if input_ids[end:end + len(self.eos_id)] == self.eos_id:
                    break
                end += 1
            # 只有 assistant 的回复部分设置真实标签
            for j in range(start, min(end + len(self.eos_id), self.max_length)):
                labels[j] = input_ids[j]  # 这部分计算损失
            i = end + len(self.eos_id)
        else:
            i += 1
    return labels
```

### 3.4 可视化 SFT 标签

```
Token:  <|im_start|> system ... <|im_end|> <|im_start|> user ... <|im_end|> <|im_start|> assistant  你好！ 我是 ...  <|im_end|>
Label:     -100       -100  ...   -100        -100      -100 ...  -100        -100         你好！ 我是 ...   <|im_end|>
                                                                                           ↑                    ↑
                                                                               只有这部分计算损失（学习回复）
```

## 4. DPO 数据集（DPODataset）

### 4.1 数据格式

DPO（直接偏好优化）需要成对的数据：一个好回答 + 一个差回答

```json
{
  "chosen": [
    {"role": "user", "content": "1+1等于多少？"},
    {"role": "assistant", "content": "1+1等于2。"}
  ],
  "rejected": [
    {"role": "user", "content": "1+1等于多少？"},
    {"role": "assistant", "content": "1+1等于3。"}
  ]
}
```

### 4.2 代码解析

```python
# dataset/lm_dataset.py 第82-132行
class DPODataset(Dataset):
    def __getitem__(self, index):
        item = self.data[index]
        chosen = item['chosen']      # 好的回答
        rejected = item['rejected']  # 差的回答

        # 分别处理两个回答
        chosen_prompt = self.tokenizer.apply_chat_template(chosen, ...)
        rejected_prompt = self.tokenizer.apply_chat_template(rejected, ...)

        # 编码
        chosen_input_ids = self.tokenizer(chosen_prompt, ...).input_ids
        rejected_input_ids = self.tokenizer(rejected_prompt, ...).input_ids

        # 生成 loss_mask（只在 assistant 回复处计算）
        chosen_loss_mask = self.generate_loss_mask(chosen_input_ids)
        rejected_loss_mask = self.generate_loss_mask(rejected_input_ids)

        return {
            'x_chosen': chosen_input_ids[:-1],      # 输入
            'y_chosen': chosen_input_ids[1:],       # 标签（错位）
            'mask_chosen': chosen_loss_mask[1:],    # 掩码
            'x_rejected': rejected_input_ids[:-1],
            'y_rejected': rejected_input_ids[1:],
            'mask_rejected': rejected_loss_mask[1:]
        }
```

### 4.3 DPO 的数据流

```
同一个问题的两个回答：

Chosen（好）:  "1+1=2" → 模型应该增加这个回答的概率
Rejected（差）: "1+1=3" → 模型应该降低这个回答的概率

DPO 通过对比学习让模型偏向好的回答
```

## 5. RLAIF 数据集（RLAIFDataset）

### 5.1 数据格式

RLAIF（AI反馈强化学习）只需要问题，回答由模型自己生成：

```json
{
  "conversations": [
    {"role": "user", "content": "请解释什么是机器学习？"}
  ]
}
```

### 5.2 代码解析

```python
# dataset/lm_dataset.py 第153-186行
class RLAIFDataset(Dataset):
    def create_chat_prompt(self, conversations):
        messages = []
        answer = ''
        for i, turn in enumerate(conversations):
            role = 'user' if i % 2 == 0 else 'assistant'
            messages.append({"role": role, "content": turn['content']})
            answer = turn['content']

        # add_generation_prompt=True 添加 assistant 的开始标记
        return self.tokenizer.apply_chat_template(
            messages[:-1],
            tokenize=False,
            add_generation_prompt=True  # 重要！
        ), answer

    def __getitem__(self, index):
        sample = self.samples[index]
        prompt, answer = self.create_chat_prompt(sample['conversations'])
        return {
            'prompt': prompt,  # 只返回提示词
            'answer': answer   # 参考答案（用于奖励计算）
        }
```

### 5.3 RLAIF 的数据流

```
数据集只提供 prompt
        ↓
模型生成多个回答（num_generations=8）
        ↓
奖励模型打分
        ↓
根据分数更新模型
```

## 6. 特殊 Token 详解

### 6.1 MiniMind 的特殊 Token

```python
# 在 tokenizer_config.json 中定义
bos_token = "<|im_start|>"  # 开始标记
eos_token = "<|im_end|>"    # 结束标记
pad_token = "<|endoftext|>" # 填充标记
```

### 6.2 各 Token 的作用

| Token | ID | 用途 |
|-------|-----|------|
| `<\|im_start\|>` | 1 | 标记消息开始 |
| `<\|im_end\|>` | 2 | 标记消息结束 |
| `<\|endoftext\|>` | 0 | 填充空白位置 |

### 6.3 为什么 PAD 位置的 label 是 -100？

```python
labels[input_ids == self.tokenizer.pad_token_id] = -100
```

PyTorch 的 `CrossEntropyLoss` 有一个参数 `ignore_index=-100`，设为 -100 的位置不会计算损失。这样：
- PAD 位置不影响训练
- 模型只学习有意义的内容

## 7. 数据加载器（DataLoader）

### 7.1 基本使用

```python
# trainer/train_pretrain.py
train_ds = PretrainDataset(data_path, tokenizer, max_length=340)
loader = DataLoader(
    train_ds,
    batch_size=32,           # 每批32个样本
    num_workers=8,           # 8个进程并行加载
    pin_memory=True          # 加速GPU传输
)
```

### 7.2 SkipBatchSampler（断点续训）

MiniMind 支持从中断处继续训练：

```python
# trainer/trainer_utils.py 第134-157行
class SkipBatchSampler(Sampler):
    def __init__(self, sampler, batch_size, skip_batches=0):
        self.skip_batches = skip_batches  # 跳过的批次数

    def __iter__(self):
        batch = []
        skipped = 0
        for idx in self.sampler:
            batch.append(idx)
            if len(batch) == self.batch_size:
                if skipped < self.skip_batches:
                    skipped += 1  # 跳过已训练的批次
                    batch = []
                    continue
                yield batch
                batch = []
```

## 8. 实际数据示例

### 8.1 预训练数据（pretrain_hq.jsonl）

```json
{"text": "人工智能（Artificial Intelligence，简称AI）是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器。"}
{"text": "机器学习是人工智能的一个子领域，专注于使计算机系统能够从数据中学习和改进，而无需进行明确的编程。"}
```

### 8.2 SFT 数据（sft_mini_512.jsonl）

```json
{
  "conversations": [
    {"role": "user", "content": "什么是人工智能？"},
    {"role": "assistant", "content": "人工智能（AI）是计算机科学的一个领域，致力于创建能够执行通常需要人类智能的任务的系统。这包括学习、推理、问题解决、感知和语言理解等能力。"}
  ]
}
```

### 8.3 DPO 数据（dpo.jsonl）

```json
{
  "chosen": [
    {"role": "user", "content": "如何学习编程？"},
    {"role": "assistant", "content": "学习编程可以从以下几步开始：1. 选择一门入门语言如Python；2. 通过在线课程学习基础语法；3. 多做练习项目；4. 阅读优秀代码；5. 加入编程社区交流。"}
  ],
  "rejected": [
    {"role": "user", "content": "如何学习编程？"},
    {"role": "assistant", "content": "编程很难，你需要很聪明才能学会。"}
  ]
}
```

## 9. 总结

| 数据集 | 用途 | 关键字段 | Label 策略 |
|--------|------|----------|-----------|
| PretrainDataset | 预训练 | text | 全部计算（除PAD） |
| SFTDataset | 监督微调 | conversations | 只计算assistant |
| DPODataset | 偏好优化 | chosen, rejected | 只计算assistant |
| RLAIFDataset | 强化学习 | conversations | 无（在线生成） |

**核心原则**：
1. 预训练学习"语言本身"，所有文本都计算损失
2. SFT/DPO 只学习"如何回复"，只在 assistant 部分计算损失
3. RLAIF 是在线学习，数据集只提供 prompt

---

[← 上一篇：模型架构详解](./02_模型架构详解.md) | [下一篇：训练流程详解 →](./04_训练流程详解.md)
